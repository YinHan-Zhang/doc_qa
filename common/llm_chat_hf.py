# -*- coding: utf-8 -*-
import json
import torch
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)
# from config.tools import tools
from typing import Union, List, Tuple
from transformers import (AutoTokenizer, AutoModel, 
                          AutoModelForCausalLM, 
                          AutoModelForSequenceClassification)


class Model:
    """
    HuggingFace æ¨¡å‹åŸºç±»
    """
    model_list = {
        "THUDM/chatglm-6b": "chatglm",
        "THUDM/chatglm2-6b": "chatglm",
        "THUDM/chatglm3-6b": "chatglm",
        "Qwen/Qwen-7B-Chat": "qwen",
        "Qwen/Qwen-14B-Chat": "qwen",
        "BAAI/bge-large-zh": "bert",
        "BAAI/bge-large-zh-v1.5": "bert",
        "BAAI/bge-reranker-large": "xlm-roberta",
        "BAAI/bge-reranker-base": "xlm-roberta"
    }
    
    @classmethod
    def get_model_type(cls, model_name_or_path: str) -> None:
        if model_name_or_path in cls.model_list:
            cls.model_type = cls.model_list[model_name_or_path]
            return
        
        with open(f'{model_name_or_path}/config.json', 'r') as f:
            data = json.load(f)
        cls.model_type = data['model_type']
        return
    
    @classmethod
    def from_pretrained(cls, 
                        model_name_or_path: str, 
                        device: str = "cuda", 
                        int_4: bool = False
                        ) -> object:
        """åŠ è½½æ¨¡å‹

        Args:
            model_name_or_path (str): æ¨¡å‹çš„ HuggingFace ä»“åº“åæˆ–æœ¬åœ°è·¯å¾„\n
            device (str, optional): åŠ è½½è®¾å¤‡, "cuda": å• GPU, "auto": å¤š GPU, "cpu": CPU. Defaults to "cuda".\n
            int_4 (bool, optional): æ˜¯å¦å¼€å¯ int4 é‡åŒ–. Defaults to False.

        Returns:
            object: æ¨¡å‹å¯¹è±¡
        """
        assert device in ["auto", "cuda", "cpu"], \
            f"`device` must be 'auto' or 'cuda', not support cpu currently. Your input is: {device}"
        
        cls.model_name_or_path = model_name_or_path
        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name_or_path, trust_remote_code=True)

        cls.get_model_type(model_name_or_path)
        if cls.model_type in ["chatglm", "bert"]:
            if device == "auto":
                if int_4:
                    cls.model = AutoModel.from_pretrained(cls.model_name_or_path, 
                                                trust_remote_code=True, device_map="auto").quantize(4)
                else:
                    cls.model = AutoModel.from_pretrained(cls.model_name_or_path, 
                                                trust_remote_code=True, device_map="auto")
            elif device == "cuda":
                if int_4:
                    assert cls.model_type != "bert", "ç›®å‰åµŒå…¥æ¨¡å‹ä¸æ”¯æŒé‡åŒ–"
                    cls.model = AutoModel.from_pretrained(cls.model_name_or_path, 
                                                trust_remote_code=True).half().quantize(4).cuda()
                else:
                    cls.model = AutoModel.from_pretrained(cls.model_name_or_path, 
                                                trust_remote_code=True).half().cuda()
            else:
                cls.model = AutoModel.from_pretrained(cls.model_name_or_path, 
                                                trust_remote_code=True).float()

            cls.model = cls.model.eval()
            
        elif cls.model_type == "qwen":
            assert int_4 == False, "Qwen æ¨¡å‹åªæ”¯æŒåŸç”Ÿ int4 æ¨¡å‹æƒé‡åŠ è½½, ä¸æ”¯æŒä»åŠç²¾åº¦è½¬æ¢, \
                                            è¯·ä¸‹è½½å¹¶åŠ è½½ Qwen åŸç”Ÿ int4 æ¨¡å‹æƒé‡."
            
            if device == "auto":
                cls.model = AutoModelForCausalLM.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True, device_map="auto", fp16=True).eval()
            elif device == "cuda":
                cls.model = AutoModelForCausalLM.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True, fp16=True).cuda().eval()
            else:
                cls.model = AutoModelForCausalLM.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True).eval()
        
        elif cls.model_type == "xlm-roberta":
            assert int_4 == False, "ç›®å‰åµŒå…¥æ¨¡å‹ä¸æ”¯æŒé‡åŒ–"
            if device == "auto":
                cls.model = AutoModelForSequenceClassification.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True, device_map="auto").eval()
            elif device == "cuda":
                cls.model = AutoModelForSequenceClassification.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True).cuda().eval()
            else:
                cls.model = AutoModelForSequenceClassification.from_pretrained(
                    cls.model_name_or_path, trust_remote_code=True).eval()
        
        else:   # TODO baichuan2
            pass
        return cls()


class LLM(Model):
    """
    å¤§è¯­è¨€æ¨¡å‹ç±»
    """
    system_prompt = {
        "chatglm": 
            '{"role": "system", "content": "Answer the following questions as best as you can. \
            You have access to the following tools:", "tools": {tools}}',
        "qwen_tool_desc": 
            "{name_for_model}: Call this tool to interact with the {name_for_human} API. \
            What is the {name_for_human} API useful for? {description_for_model} Parameters: {parameters} \
            Format the arguments as a JSON object.",
        "qwen_react_prompt":
            "Answer the following questions as best you can. You have access to the following tools:\
            \n\n{tool_descs}\n\nUse the following format:\n\nQuestion: the input question you must answer\
            \nThought: you should always think about what to do\
            \nAction: the action to take, should be one of [{tool_names}]\
            \nAction Input: the input to the action\nObservation: the result of the action\
            \n... (this Thought/Action/Action Input/Observation can be repeated zero or more times)\
            \nThought: I now know the final answer\
            \nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {query}"
        }

    def get_qwen_prompt(self, query: str, tools: list) -> str:
        tool_descs, tool_names = [], []

        for info in tools:
            tool_descs.append(
                self.system_prompt["qwen_tool_desc"].format(
                    name_for_model=info['name_for_model'],
                    name_for_human=info['name_for_human'],
                    description_for_model=info['description_for_model'],
                    parameters=json.dumps(
                        info['parameters'], ensure_ascii=False),
                )
            )
            tool_names.append(info['name_for_model'])
            
        tool_descs = '\n\n'.join(tool_descs)
        tool_names = ','.join(tool_names)
        prompt = self.system_prompt["qwen_react_pormpt"].format(
            tool_descs=tool_descs, tool_names=tool_names, query=query)
        return prompt
    
    def chat_completion(self, 
                        query: str, 
                        history: List[str,] = [], 
                        tools: list = None,
                        observation: bool = False
                        ) -> Tuple[str, List[str,]]:
        """å¯¹è¯

        Args:
            query (str): ç”¨æˆ·æé—®\n
            history (List[str,]): å†å²å¯¹è¯, é»˜è®¤ä¸ºç©º\n
            tools (list, optional): å¯ç”¨å·¥å…·åˆ—è¡¨, é»˜è®¤ä¸º None\n
            observation (bool, optional): `query` æ˜¯å¦ä¸º tool è¿”å›å€¼, é»˜è®¤ä¸º False

        Returns:
            Tuple[str, List[str,]]: response, history
        """
        assert type(query) == str, f"`query` must be str, not {type(query)}"
        
        if self.model_type == "chatglm":   # ChatGLM
            if tools:
                history = [eval(self.system_prompt["chatglm"].format(tools=tools))] + history
                
            response, history = self.model.chat(self.tokenizer, 
                                                query, 
                                                history=history,
                                                role="observation" if observation else "user")
            
        elif self.model_type == "qwen":   # Qwen
            if observation:
                react_stop_words = [
                    # self.tokenizer.encode('Observation'),  # [37763, 367]
                    self.tokenizer.encode('Observation:'),  # [37763, 367, 25]
                    self.tokenizer.encode('Observation:\n'),  # [37763, 367, 510]
                ]
                self.prompt += str(query)
                response, history = self.model.chat(self.tokenizer,
                                                    self.prompt, 
                                                    history, 
                                                    stop_words_ids=react_stop_words)
            else:
                if tools:
                    self.prompt = self.get_qwen_prompt(query, tools)
                    query = self.prompt
                    
                response, history = self.model.chat(self.tokenizer,
                                                    query, 
                                                    history=history)
        else:   # TODO baichuan2
            pass
        
        return response, history


class Embedding(Model):
    """
    åŒå¡”åµŒå…¥æ¨¡å‹ç±»
    """
    def get_text_embedding(self, text_list: List[str,]) -> torch.float16:
        """è·å–æ–‡æœ¬åµŒå…¥

        Args:
            text_list (List[str,]): æ–‡æœ¬åˆ—è¡¨

        Returns:
            torch.float16: æ–‡æœ¬åµŒå…¥, shape: (n_texts, embedding_dim)
        """
        encoded_input = self.tokenizer(text_list, 
                                       padding=True, 
                                       truncation=True, 
                                       return_tensors='pt'
                                       ).to(self.model.device)

        with torch.no_grad():
            model_output = self.model(**encoded_input)
            sentence_embeddings = model_output[0][:, 0]

        sentence_embeddings = torch.nn.functional.normalize(
            sentence_embeddings, p=2, dim=1)
        
        return sentence_embeddings


class Reranker(Model):
    """
    å•å¡”é‡æ’åºåµŒå…¥æ¨¡å‹ç±»
    """
    def get_pair_score(self, pairs: List[List[str,],]) -> torch.float32:
        """è·å–é—®ç­”å¯¹ç›¸ä¼¼åº¦åˆ†æ•°

        Args:
            pairs (List[List[str, str],]): é—®ç­”å¯¹, å¦‚:   
                `[['ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ', 'æˆ‘å«å°æ˜ã€‚'], ['ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ', 'ä¸‰ä½“æ˜¯ç”±åˆ˜æ…ˆæ¬£æ‰€è‘—çš„é•¿ç¯‡ç§‘å¹»å°è¯´ã€‚']]`

        Returns:
            torch.float32: é—®ç­”å¯¹çš„ç›¸ä¼¼åº¦åˆ†æ•°, shape: (n_pairs,)
        """
        with torch.no_grad():
            inputs = self.tokenizer(pairs, 
                                    padding=True, 
                                    truncation=True, 
                                    return_tensors='pt', 
                                    max_length=512
                                    ).to(self.model.device)
            scores = self.model(**inputs, return_dict=True).logits.view(-1, ).float()
        
        return scores


if __name__ == "__main__":
    # å¤§æ¨¡å‹é—®ç­”
    # TODO tools è°ƒç”¨çš„ä¾‹å­
    model = LLM.from_pretrained("models/qwen-14b")
    outs = model.chat_completion("ä½ å¥½")
    print(outs)
    # ('ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚', 
    #   [{'role': 'user', 'content': 'ä½ å¥½'}, 
    #    {'role': 'assistant', 'metadata': '', 'content': 'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'}
    #   ]
    # )
    
    # æ–‡æœ¬åµŒå…¥
    embedding = Embedding.from_pretrained("models/bge-large-zh")
    outs = embedding.get_text_embedding(["ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜", "ä»Šå¹´æ˜¯2024å¹´"])
    print(outs, outs.shape)
    # tensor([[ 0.0271, -0.0481, -0.0304,  ..., -0.0145, -0.0151,  0.0016],
    #    [ 0.0103,  0.0007, -0.0489,  ...,  0.0454,  0.0270, -0.0220]], device='cuda:0', dtype=torch.float16) 
    # torch.Size([2, 1024])
    
    # é—®ç­”å¯¹é‡æ’åºç›¸ä¼¼åº¦
    reranker = Reranker.from_pretrained("models/bge-reranker-large")
    outs = reranker.get_pair_score([["ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ", "æˆ‘å«å°æ˜ã€‚"], ["ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ", "ä¸‰ä½“æ˜¯ç”±åˆ˜æ…ˆæ¬£æ‰€è‘—çš„é•¿ç¯‡ç§‘å¹»å°è¯´ã€‚"]])
    print(outs)
    # tensor([-1.8043, -9.4000], device='cuda:0')